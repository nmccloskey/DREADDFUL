#!/usr/bin/env python3

"""
calibrate_cellpose.py â€” config-driven grid search for Cellpose v4 (SAM)

Minimal CLI (Windows-friendly):
  python calibrate_cellpose.py --config calib_config.json [--gpu]

See the JSON example written alongside this script for the config shape.

Author: Nick McCloskey
The initial structure for this script was generated by ChatGPT (OpenAI, version 5, Aug 2025),
then adapted and refined by the author for specific project needs.
"""

import argparse, os, json, warnings
from datetime import datetime
from itertools import product
from pathlib import Path
import numpy as np
import pandas as pd
from PIL import Image, ImageOps
from tqdm.auto import tqdm
from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, f1_score
from skimage.measure import label, regionprops
from skimage.morphology import binary_dilation, disk
from skimage.filters import laplace
from cellpose import models

def imread_any(path):
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        im = Image.open(path)
        im = ImageOps.exif_transpose(im)
        return np.array(im)

def read_roi_for(slice_path, provided=None):
    p = Path(slice_path)
    if provided and isinstance(provided, str) and provided.strip() and os.path.exists(provided):
        return (np.array(Image.open(provided)) > 0)
    for ext in (".tif",".tiff",".png",".jpg",".jpeg"):
        guess = p.with_name(p.stem + "_mask" + ext)
        if guess.exists():
            return (np.array(Image.open(guess)) > 0)
    raise FileNotFoundError(f"No ROI mask found for {slice_path}")

def run_cellpose(model, img, roi, P):
    # choose channel_axis based on image
    if img.ndim == 2:
        channel_axis = None
    elif img.ndim == 3 and img.shape[-1] in (3,4,1):
        if img.shape[-1] == 4:
            img = img[..., :3]
        if img.shape[-1] == 1:
            img = img[..., 0]
            channel_axis = None
        else:
            channel_axis = -1
    else:
        raise ValueError(f"Unsupported image shape: {img.shape}")
    diameter = None if (P['diameter'] in (None, 0)) else float(P['diameter'])
    # Cellpose v4 returns (masks, flows, styles). Avoid 'resample' to silence resize deprecation.
    masks, flows, styles = model.eval(
        img,
        channel_axis=channel_axis,
        normalize=True, invert=False,
        diameter=diameter,
        flow_threshold=float(P['flow_threshold']),
        cellprob_threshold=float(P['cellprob_threshold']),
        do_3D=False, compute_masks=True,
        min_size=int(P['min_size']),
        max_size_fraction=float(P['max_size_fraction']),
        niter=None if str(P['niter']).lower() == "auto" else int(P['niter']),
        tile_overlap=float(P['tile_overlap']),
    )
    # ROI restriction and relabel
    labels_img = masks.copy()
    labels_img[~roi] = 0
    labels_img = label(labels_img > 0, connectivity=1)
    return labels_img

def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    cfg.setdefault("threshold", 150.0)
    cfg.setdefault("pretrained", "cpsam")
    if "grid" not in cfg:
        raise ValueError("Config must include a 'grid' section with parameter arrays.")
    # required grid keys
    keys = ["diameter","cellprob_threshold","flow_threshold","min_size","max_size_fraction","niter","tile_overlap"]
    for k in keys:
        if k not in cfg["grid"] or not isinstance(cfg["grid"][k], list):
            raise ValueError(f"Grid must include list for '{k}'.")
    return cfg

def pick_pixel_size(meta_row):
    # try a few common column names for microns per pixel (um/px)
    for key in ["um_per_px","micron_per_pixel","pixel_size_um","um_per_pix"]:
        if key in meta_row and not pd.isna(meta_row[key]):
            return float(meta_row[key])
    # or px per um
    for key in ["px_per_um","pix_per_um"]:
        if key in meta_row and not pd.isna(meta_row[key]) and float(meta_row[key]) != 0:
            return 1.0/float(meta_row[key])
    return None  # unknown

def qc_metrics(img, roi, labels_img):
    roi_px = int(roi.sum())
    fg_px = int((labels_img > 0).sum())
    coverage = (fg_px / roi_px) if roi_px > 0 else np.nan
    # object areas
    props = regionprops(labels_img)
    areas = np.array([p.area for p in props]) if len(props) else np.array([])
    med_area = float(np.median(areas)) if len(areas) else np.nan
    # simple SNR proxy: ROI mean / std of a dilated rim outside ROI
    rim = binary_dilation(roi, disk(30)) & (~roi)
    rim_vals = img[rim] if img.ndim == 2 else img[...,0][rim]
    roi_vals = img[roi] if img.ndim == 2 else img[...,0][roi]
    snr = float(np.mean(roi_vals) / (np.std(rim_vals) + 1e-6)) if rim_vals.size and roi_vals.size else np.nan
    # focus/blur metric: variance of Laplacian within ROI
    lap = laplace(img if img.ndim==2 else img[...,0])
    lap_var = float(np.var(lap[roi])) if roi_px>0 else np.nan
    return roi_px, coverage, med_area, snr, lap_var

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    ap.add_argument("--gpu", action="store_true")
    args = ap.parse_args()

    cfg = load_config(args.config)
    meta = pd.read_csv(cfg["meta"])

    # timestamped subdir
    out_base = cfg["out_dir"]
    os.makedirs(out_base, exist_ok=True)
    stamp = datetime.now().strftime("%y%m%d_%H%M")
    out_dir = os.path.join(out_base, f"cellpose_calibrations_{stamp}")
    os.makedirs(out_dir, exist_ok=True)

    # optional subset
    if isinstance(cfg.get("subset"), dict) and len(cfg["subset"].get("slices", [])):
        targets = set(cfg["subset"]["slices"])
        def keep(row):
            p = str(row["slice_path"]); st = Path(p).stem
            return (p in targets) or (st in targets)
        meta = meta[meta.apply(keep, axis=1)].reset_index(drop=True)

    model = models.CellposeModel(gpu=args.gpu, pretrained_model=cfg.get("pretrained","cpsam"))

    G = cfg["grid"]
    combos = list(product(G["diameter"], G["cellprob_threshold"], G["flow_threshold"],
                          G["min_size"], G["max_size_fraction"], G["niter"], G["tile_overlap"]))
    settings = [dict(diameter=d, cellprob_threshold=cp, flow_threshold=fl, min_size=ms,
                     max_size_fraction=mf, niter=ni, tile_overlap=to)
                for (d,cp,fl,ms,mf,ni,to) in combos]

    per_rows = []
    pbar = tqdm(total=len(settings) * len(meta), desc="Calibrating (param x images)")

    for sid, P in enumerate(settings):
        for _, row in meta.iterrows():
            try:
                img = imread_any(row["slice_path"])
                roi = read_roi_for(row["slice_path"], row.get("mask_path",""))
                labels_img = run_cellpose(model, img, roi, P)

                # counts & QC
                n = int(labels_img.max())
                roi_px, coverage, med_area, snr, lap_var = qc_metrics(img, roi, labels_img)

                # densities (pixels); physical if pixel size available
                um_per_px = pick_pixel_size(row)
                density_px = (n / roi_px * 1e5) if roi_px>0 else np.nan  # cells per 1e5 px
                if um_per_px:
                    area_mm2 = roi_px * (um_per_px**2) / 1e6
                    density_mm2 = n / area_mm2 if area_mm2>0 else np.nan
                else:
                    area_mm2 = np.nan; density_mm2 = np.nan

                m = row.get("manual_count", np.nan)
                rec = {
                    "setting_id": sid,
                    "animal": row["animal"],
                    "group": row["group"],
                    "slice": row["slice_path"],
                    "manual_count": float(m) if pd.notna(m) else np.nan,
                    "auto_count": n,
                    "roi_area_px": roi_px,
                    "coverage": coverage,
                    "median_obj_area": med_area,
                    "snr_roi_rim": snr,
                    "laplacian_var": lap_var,
                    "density_per_1e5_px": density_px,
                    "roi_area_mm2": area_mm2,
                    "density_per_mm2": density_mm2,
                    **P
                }
                if pd.notna(rec["manual_count"]):
                    diff = rec["auto_count"] - rec["manual_count"]
                    rec["abs_diff"] = abs(diff)
                    rec["pct_diff"] = 100.0 * abs(diff) / max(1.0, rec["manual_count"])
                    rec["hit_manual"] = int(rec["manual_count"] >= cfg["threshold"])
                    rec["hit_auto"] = int(rec["auto_count"] >= cfg["threshold"])
                per_rows.append(rec)
            except Exception as e:
                per_rows.append({
                    "setting_id": sid,
                    "animal": row.get("animal",""),
                    "group": row.get("group",""),
                    "slice": row.get("slice_path",""),
                    "manual_count": float(row.get("manual_count", np.nan)),
                    "auto_count": np.nan,
                    **P,
                    "error": f"{type(e).__name__}: {e}"
                })
            finally:
                pbar.update(1)
    pbar.close()

    per_df = pd.DataFrame(per_rows)
    per_df.to_csv(os.path.join(out_dir, "per_slice.csv"), index=False)

    # Per-setting summary (including QC aggregations)
    summaries = []
    for sid, g in per_df.groupby("setting_id"):
        rec = {k: g[k].iloc[0] for k in ["diameter","cellprob_threshold","flow_threshold","min_size","max_size_fraction","niter","tile_overlap"]}
        rec["n_images"] = int(len(g))
        g_scored = g.dropna(subset=["auto_count","manual_count"])
        rec["n_scored"] = int(len(g_scored))
        if len(g_scored):
            rec["mape"] = float(np.mean(g_scored["pct_diff"]))
            rec["mae"] = float(np.mean(g_scored["abs_diff"]))
            y_true = g_scored["hit_manual"].astype(int).values
            y_pred = g_scored["hit_auto"].astype(int).values
            rec["bal_acc"] = float(balanced_accuracy_score(y_true, y_pred))
            rec["mcc"] = float(matthews_corrcoef(y_true, y_pred))
            rec["f1"] = float(f1_score(y_true, y_pred))
        else:
            rec["mape"] = rec["mae"] = rec["bal_acc"] = rec["mcc"] = rec["f1"] = np.nan
        # QC aggregates (medians)
        for col in ["coverage","median_obj_area","snr_roi_rim","laplacian_var","density_per_1e5_px","density_per_mm2"]:
            if col in g:
                rec[f"{col}_median"] = float(np.nanmedian(g[col].values))
        summaries.append(rec)

    summ_df = pd.DataFrame(summaries).sort_values(["mape","bal_acc"], na_position="last")
    summ_df.to_csv(os.path.join(out_dir, "per_setting.csv"), index=False)

    # Save "winners"
    best = {}
    s1 = summ_df.dropna(subset=["mape"])
    if len(s1): best["best_by_mape"] = s1.sort_values("mape").head(1).to_dict(orient="records")[0]
    s2 = summ_df.dropna(subset=["bal_acc"])
    if len(s2): best["best_by_balacc"] = s2.sort_values("bal_acc", ascending=False).head(1).to_dict(orient="records")[0]
    with open(os.path.join(out_dir, "best.json"), "w", encoding="utf-8") as f:
        json.dump(best, f, indent=2)

if __name__ == "__main__":
    main()
